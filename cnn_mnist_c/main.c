#include <msp430.h> 
#include <stdint.h>

#pragma PERSISTENT(q_conv_kernel)
int16_t q_conv_kernel[2][5][5] = {{{69,-17,45,-39,7},{-57,29,-24,32,-29},{30,-66,-45,-23,-13},{11,-47,33,-5,21},{27,53,-66,5,30}},{{61,-28,-62,60,48},{10,-25,23,50,66},{-15,54,-26,-31,-19},{-6,47,47,20,36},{68,36,30,70,-5}}};
#pragma PERSISTENT(q_bias)
int16_t q_bias[2] = {-8,0};
#pragma PERSISTENT(q_kernel_2)
int16_t q_kernel_2[2][2][5][5] = {{{{28,-19,-27,50,-15},{8,-53,0,11,-29},{-61,-18,12,-19,61},{11,47,-44,-6,55},{44,22,18,-23,14}},{{-14,42,-55,42,-70},{-33,-56,-20,-42,55},{-38,19,18,42,55},{-55,-54,25,25,60},{-10,-19,55,15,-51}}},{{{7,-46,10,-20,14},{-25,43,38,59,20},{12,41,31,54,-44},{-15,-56,-40,59,29},{12,-22,29,58,-59}},{{-54,53,-13,-33,-59},{-60,-28,10,56,-47},{-11,-39,58,24,-54},{-14,53,-12,-31,42},{-41,-42,-40,16,-1}}}};
#pragma PERSISTENT(q_bias_2)
int16_t q_bias_2[2] = {0,-5};
#pragma PERSISTENT(q_dense_weights_1)
int16_t q_dense_weights_1[98][10] = {{37,39,18,-39,39,59,-36,-43,-47,-3},{17,14,59,41,0,-38,-14,33,-32,21},{-38,-53,20,-55,-36,-9,-5,-51,-22,4},{41,33,-24,-28,-22,-3,-52,33,9,42},{-21,49,50,56,-29,-41,-44,-15,-13,12},{-30,-9,30,57,-32,-51,41,-10,-34,52},{59,41,16,-48,-33,-15,-55,-25,15,-55},{-40,-1,-50,11,0,-37,15,35,19,38},{-41,-50,48,48,3,-30,45,18,54,-17},{-31,-10,32,48,12,-59,17,27,27,-23},{-18,-39,60,23,57,28,-44,-5,-24,-22},{-11,26,-37,50,-47,-46,-52,47,29,8},{26,-29,2,62,-48,56,55,38,-8,-42},{50,-3,52,53,-48,-61,-26,47,29,-49},{31,10,-42,-51,-41,-55,26,-56,36,16},{-32,-22,18,21,38,-49,-26,-55,-32,13},{-27,-25,-31,52,1,-59,48,34,40,45},{2,52,-24,-22,27,-52,-1,35,-42,13},{3,58,-17,37,11,12,-15,10,62,49},{-55,2,-17,58,15,43,24,40,46,36},{-13,-27,-35,-51,34,-59,-7,-41,-23,-55},{-18,23,30,-41,-25,51,-38,-26,-10,-35},{-34,22,40,59,30,-43,-2,42,-30,-31},{33,-50,30,-59,-59,-6,-26,13,24,7},{30,39,30,-58,11,-51,49,-21,32,-36},{22,11,-57,44,47,-47,-2,-19,15,9},{-46,8,22,25,-13,1,3,-59,11,-14},{37,-45,24,-20,-10,42,-18,44,-4,-41},{20,48,23,-4,-13,-15,-59,-14,56,-48},{11,-51,-18,1,-40,39,27,39,-54,-15},{-49,-37,-18,21,-4,-11,-36,48,-7,-20},{42,-17,-17,-9,43,12,11,-16,16,36},{-32,-54,44,-2,-36,26,-49,-1,-66,-13},{-59,38,-30,-42,-6,42,26,26,43,19},{-53,-59,64,-47,-22,59,-39,-22,40,-46},{-20,-29,-14,55,-18,-20,-9,2,54,-13},{33,-16,-4,-5,46,42,0,-38,-34,28},{-60,8,33,33,-54,36,-8,-26,-4,-49},{16,14,-14,34,-28,25,-16,-31,30,-2},{-21,29,-25,-51,-47,-34,-54,14,0,-4},{-44,-39,31,-41,48,32,-7,-23,64,8},{36,0,-30,10,44,24,23,-32,22,-33},{35,-15,1,-19,49,-55,-8,53,-4,22},{40,-33,53,-16,27,17,0,-46,13,46},{-23,49,38,58,4,12,-54,46,-10,1},{-41,35,-49,3,56,-47,57,-44,-55,-56},{48,-23,34,-11,50,-18,28,19,-43,-4},{2,-7,-56,-32,39,-24,43,-6,-46,-38},{-38,37,-30,12,-24,29,25,-26,-4,14},{30,26,11,-53,50,-22,-16,39,51,21},{-54,50,33,-52,-33,-6,-18,-37,-45,-14},{20,2,-51,59,14,-21,58,-24,19,-20},{50,-53,0,-44,3,-25,39,25,-51,-47},{-59,28,45,32,51,12,-24,-41,49,-7},{-56,-13,-47,-34,-21,44,49,51,-48,-37},{54,-52,24,-33,-3,-17,-38,4,-40,53},{28,39,-1,-53,-58,-2,-3,22,-57,-16},{41,-52,-3,30,-32,-47,-9,-15,17,1},{1,12,14,12,1,28,23,50,22,26},{22,-2,33,-4,-17,33,-8,19,27,-12},{-26,27,-20,-52,-58,-41,-34,-18,0,27},{23,35,39,2,-46,48,8,60,-38,57},{54,17,-12,-33,4,-42,-55,-41,38,39},{33,34,1,-16,45,2,34,-41,-22,-14},{-59,49,5,-9,51,4,54,44,-15,-34},{30,-54,-25,53,33,-14,-5,-55,26,-22},{-58,33,59,-50,-58,53,46,43,-40,-52},{-39,34,-26,-17,-44,-22,-7,5,59,15},{40,44,-43,46,-42,39,-57,-39,-42,50},{-33,-38,-38,-53,-16,48,42,-55,32,6},{-17,17,-49,-27,-53,14,-42,-11,41,46},{53,-16,-54,-52,-4,-47,-17,50,-9,-2},{3,17,-28,-40,-35,40,41,29,15,-3},{19,-46,-55,-56,-43,-15,26,4,21,-12},{-3,27,-53,42,8,15,-2,0,-9,33},{26,40,-15,52,-44,-51,7,36,57,55},{6,-53,54,-33,5,53,16,0,14,40},{12,43,-30,22,44,-8,-1,-43,33,-32},{27,16,-8,57,-55,-49,56,-56,-28,45},{-44,-30,4,-38,38,8,18,11,-57,42},{0,6,25,-18,-17,-13,-37,-54,46,-57},{-10,-22,20,-35,19,-3,-38,60,-29,-29},{-3,25,-58,37,42,44,-12,-59,-31,-3},{-59,32,-33,-46,13,-24,9,-5,42,48},{-23,-4,-21,-31,58,47,-50,36,-21,-5},{-19,-4,1,-27,-22,18,23,3,31,-3},{-30,-29,53,9,7,11,54,-9,17,36},{-8,43,5,-22,-43,55,46,-31,49,29},{49,39,-31,-45,4,-26,29,20,-6,-54},{-41,9,-4,59,0,-6,-47,46,0,6},{0,-7,11,-6,-56,17,-42,-47,4,-11},{-57,-7,-27,7,-59,12,30,-56,24,3},{57,21,-51,4,54,-8,-36,-34,38,-14},{-43,13,12,29,-47,-52,15,22,32,-51},{-39,-51,8,35,35,31,59,27,-27,12},{53,-17,27,-18,21,-23,3,30,-16,29},{28,-25,-52,-38,61,53,-36,39,60,-53},{-51,2,-39,18,24,-38,-31,-27,36,50}};
#pragma PERSISTENT(q_dense_bias_1)
int16_t q_dense_bias_1[10] = {0,1,-8,15,8,-14,3,-5,1,-12};
#pragma PERSISTENT(q_dense_weights_2)
int16_t q_dense_weights_2[10][10] = {{-105,-1,38,136,7,38,78,-92,104,13},{-129,46,68,87,23,-82,-104,-113,-49,-106},{21,18,45,79,-136,-25,117,104,-56,-113},{-122,-31,83,-73,-62,-24,-33,43,-67,-70},{-55,-45,30,101,-16,25,-97,-111,87,-18},{69,114,43,133,-101,26,9,9,-117,59},{57,72,-73,128,-23,68,36,-115,-57,65},{11,44,88,63,98,122,116,-19,50,-24},{3,-83,80,86,-127,-9,82,-41,-111,-33},{123,-128,-67,-79,-92,-43,72,36,129,-88}};
#pragma PERSISTENT(q_dense_bias_2)
int16_t q_dense_bias_2[10] = {4,11,-13,-14,6,-9,-2,9,3,3};

#pragma PERSISTENT(q_in)
int16_t q_in[28][28] = {{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,133,254,160,9,54,136,29,195,256,254,254,254,99,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,136,249,253,253,207,253,253,253,253,253,253,253,253,152,0,0},{0,0,0,0,0,0,0,0,0,0,15,95,244,253,253,253,253,253,253,253,201,200,200,200,107,50,0,0},{0,0,0,0,0,0,0,0,0,0,105,253,253,253,253,253,253,232,138,65,2,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,86,242,253,253,251,164,164,82,33,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,82,244,253,253,253,198,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,18,249,253,253,253,251,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,7,131,253,253,253,157,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,14,182,253,253,253,161,19,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,16,110,243,253,253,200,50,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,48,195,253,253,244,95,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,17,101,253,253,244,94,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,1,100,253,253,176,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,253,253,176,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,23,127,17,0,93,253,253,176,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,113,231,237,24,24,163,253,253,176,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,171,253,187,66,213,253,253,208,76,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,171,253,253,253,253,253,240,52,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,104,228,253,253,253,244,73,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,102,135,190,78,16,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}};

#pragma PERSISTENT(q_conv_activate_out)
int16_t q_conv_activate_out[2][28][28] = {0};
#pragma PERSISTENT(q_pool_out)
int16_t q_pool_out[2][14][14] = {0};
#pragma PERSISTENT(q_ca_out_2)
int16_t q_ca_out_2[2][14][14] = {0};
#pragma PERSISTENT(q_pool_out_2)
int16_t q_pool_out_2[2][7][7] = {0};
#pragma PERSISTENT(q_flattened)
int16_t q_flattened[98] = {0};
#pragma PERSISTENT(q_dense_out_1)
int16_t q_dense_out_1[10] = {0};
#pragma PERSISTENT(q_dense_out_2)
int16_t q_dense_out_2[10] = {0};


int16_t mul(int16_t x, int16_t y){
  int32_t res = (int32_t) x * (int32_t) y;
  return (int16_t)(res >> 8);
}

/*
 * main.c
 */
int main(void) {
    WDTCTL = WDTPW | WDTHOLD;	// Stop watchdog timer


    int i, j, k, ki, kj, layer;

    // First conv-activate layer
    for(k=0;k<2;k++){
      for(i=0;i<28;i++){
        for(j=0;j<28;j++){
          int16_t q_mac = 0;
          // Apply the kernel with the center at (i,j)
          for(ki=0;ki<5;ki++){
            for(kj=0;kj<5;kj++){
              int ai = i - 2 + ki;
              int aj = j - 2 + kj;
              if(ai < 0 || aj < 0 || ai >= 28 || aj >= 28){
                continue;
              }
              q_mac += mul(q_in[ai][aj], q_conv_kernel[k][4-ki][4-kj]);
            }
          }
          q_conv_activate_out[k][i][j] = q_mac + q_bias[k];

          // Apply ReLU
          if(q_conv_activate_out[k][i][j] < 0) q_conv_activate_out[k][i][j] = 0;
        }
      }
    }

    // 2 x 2 max-pool
    for(k=0;k<2;k++){
      for(i=0;i<14;i++){
        for(j=0;j<14;j++){
          int16_t q_max_val = 0;
          for(ki=0;ki<2;ki++){
            for(kj=0;kj<2;kj++){
              int ai = i * 2 + ki;
              int aj = j * 2 + kj;
              if(q_max_val < q_conv_activate_out[k][ai][aj]) q_max_val = q_conv_activate_out[k][ai][aj];
            }
          }

          q_pool_out[k][i][j] = q_max_val;
        }
      }
    }

    // Second conv-activate layer
    for(k=0;k<2;k++){
      for(i=0;i<14;i++){
        for(j=0;j<14;j++){
          int16_t q_mac = 0;

          for(layer=0;layer<2;layer++){
            // Apply the kernel with the center at (i,j)
            for(ki=0;ki<5;ki++){
              for(kj=0;kj<5;kj++){
                int ai = i - 2 + ki;
                int aj = j - 2 + kj;
                if(ai < 0 || aj < 0 || ai >= 14 || aj >= 14){
                  continue;
                }

                q_mac += mul(q_pool_out[layer][ai][aj], q_kernel_2[k][layer][4-ki][4-kj]);
              }
            }

          }
          q_ca_out_2[k][i][j] = q_mac + q_bias_2[k];
          // Apply ReLU
          if(q_ca_out_2[k][i][j] < 0) q_ca_out_2[k][i][j] = 0;
        }
      }
    }

    // Second pooling layer
    for(k=0;k<2;k++){
      for(i=0;i<7;i++){
        for(j=0;j<7;j++){
          int16_t q_max_val = 0;
          for(ki=0;ki<2;ki++){
            for(kj=0;kj<2;kj++){
              int ai = i * 2 + ki;
              int aj = j * 2 + kj;
              if(q_max_val < q_ca_out_2[k][ai][aj]) q_max_val = q_ca_out_2[k][ai][aj];
            }
          }
          q_pool_out_2[k][i][j] = q_max_val;
        }
      }
    }

    // Flatten
    int qp = 0;
    for(i=0;i<2;i++){
      for(j=0;j<7;j++){
        for(k=0;k<7;k++){
          q_flattened[qp++] = q_pool_out_2[i][j][k];
        }
      }
    }


    // Compute first dense layer output (with ReLU)
    for(i=0;i<10;i++){
      int16_t q_mac = q_dense_bias_1[i];
      for(j=0;j<98;j++){
        q_mac += mul(q_flattened[j], q_dense_weights_1[j][i]);
      }
      q_dense_out_1[i] = q_mac > 0 ? q_mac : 0;
    }

    // Compute second dense layer output (without activation)
    for(i=0;i<10;i++){
      int16_t q_mac = q_dense_bias_2[i];
      for(j=0;j<10;j++){
        q_mac += mul(q_dense_out_1[j], q_dense_weights_2[j][i]);
      }
      q_dense_out_2[i] = q_mac;
    }

    int q_argmax = 0;
    for(i=0;i<10;i++){
      if(q_dense_out_2[i] > q_dense_out_2[q_argmax]) q_argmax = i;
    }


	return 0;
}
